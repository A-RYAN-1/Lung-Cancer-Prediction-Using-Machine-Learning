# -*- coding: utf-8 -*-
"""Final_PBL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YK8pcYX4xsJJDpVwLZsCR238gZwWgsAw

# Data Loading from drive
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

Data = "/content/drive/My Drive/Lung_cancer/cancer_data.csv"
df = pd.read_csv(Data)

pd.set_option('display.max_columns',None)
df.head()

df.columns

df.info()

"""# Data Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns

"""Let us see how are target variable is distributed in the dataset"""

tdf=df['Level'].value_counts().reset_index()
plt.figure(figsize=(15,5))
plt.pie(x=tdf['count'],labels=tdf['Level'],autopct='%.2f%%')
plt.legend(tdf['Level'])
plt.show()

"""1.   Our classes in the target variable is distributed almost equally with no bias towards one particular class.

2.  Our Dataset is balanced :)

Let us look how some of the Risk, Allergies and behavious are related to each other
"""

corr=df[['Air Pollution', 'Alcohol use', 'Dust Allergy', 'Genetic Risk',
            'chronic Lung Disease', 'Balanced Diet', 'Obesity', 'Smoking']].corr()
plt.figure(figsize=(15,5))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap of Health Risks')
plt.show()

plt.figure(figsize=(15, 9))
sns.boxplot(data=df,x='chronic Lung Disease',y='Age',hue='Level')
plt.title('Age Distribution by Chronic Lung Disease')
plt.xlabel('Chronic Lung Disease Level')
plt.ylabel('Age')
plt.show()
plt.figure(figsize=(15, 5))
sns.countplot(data=df,x='OccuPational Hazards',hue='Level')
plt.title('Chronic Lung Disease Prevalence by Occupational Hazards')
plt.xlabel('Occupational Hazards')
plt.ylabel('Count')
plt.legend(title='Chronic Lung Disease')
plt.show()

fig,axes=plt.subplots(nrows=3, ncols=2, figsize=(15, 15))
sns.histplot(df['Age'], bins=10, kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Histogram of Age')
sns.histplot(df['Air Pollution'], bins=10, kde=True, ax=axes[0, 1])
axes[0, 1].set_title('Histogram of Air Pollution')
sns.histplot(df['Alcohol use'], bins=2, kde=True, ax=axes[1, 0])
axes[1, 0].set_title('Histogram of Alcohol Use')
sns.countplot(x='Gender', data=df, ax=axes[1, 1])
axes[1, 1].set_title('Count Plot of Gender')
sns.countplot(x='Dust Allergy', data=df, ax=axes[2, 0])
axes[2, 0].set_title('Count Plot of Dust Allergy')
sns.countplot(x='Smoking', data=df, ax=axes[2, 1])
axes[2, 1].set_title('Count Plot of Smoking')
plt.tight_layout()
plt.show()

"""# Data Preprocessing"""

df.info()

df.isnull().any()

col=['index','Patient Id']
df.drop(columns=col,inplace=True,axis=1)

df['Level']=df['Level'].apply(lambda x: 0 if x=='Low' else 1 if x=='Medium' else 2)

df.info()

"""Finding correlation between features"""

plt.figure(figsize=(20,15))
sns.heatmap(df.corr(),annot=True,cbar='magma')
plt.show()

# Splitting the Target column from the original dataset
x=df.drop(columns='Level')
y=df['Level']

# Calculating Mutual Information for capturing both linear and non-linear relationship
from sklearn.feature_selection import mutual_info_classif
mi=mutual_info_classif(x,y)
mi_df=pd.DataFrame({'Feature': x.columns, 'Mutual Information': mi})
mi_df=mi_df.sort_values(by='Mutual Information', ascending=False).reset_index(drop=True)
print(mi_df)

"""Plotting Mutual Information for better understanding"""

plt.figure(figsize=(10,6))
sns.barplot(x='Mutual Information',y='Feature', data=mi_df)
plt.title('Mutual Information of Features for Classification')
plt.xlabel('Mutual Information')
plt.ylabel('Features')
plt.show()

"""Splitting the data into Training (75%) and Testing (25%)"""

from sklearn.model_selection import train_test_split
x_t,x_te,y_t,y_te=train_test_split(x,y,test_size=0.25,random_state=20)

"""importing"""

#importing
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report,confusion_matrix

cm_t=[] #To store confusion matrices for training set
cm=[] #To store confusion matrices for testing set

"""# Models

# KNN neighbors
"""

knn=KNeighborsClassifier()
params={'n_neighbors':list(np.arange(2,50))}
nknn=RandomizedSearchCV(knn,n_jobs=-1,random_state=20,scoring='accuracy',param_distributions=params,cv=10) #Finding Best K
nknn.fit(x_t,y_t)
print(nknn.best_params_)
print(nknn.best_score_)
nknn=nknn.best_estimator_

pred_t=nknn.predict(x_t)
pred=nknn.predict(x_te)  #Predicting on the testing set
print('KNN Training Accuracy: ',accuracy_score(y_t,pred_t))
print('KNN Testing Accuracy: ',accuracy_score(y_te,pred))
print('\nClassification Report for KNN Classifier:\n\n ',classification_report(y_te,pred))
cmk_t=confusion_matrix(y_t,pred_t)
cm_t.append(cmk_t)
cmk=confusion_matrix(y_te,pred)
cm.append(cmk)

"""# Decision Tree"""

dt=DecisionTreeClassifier()
path=dt.cost_complexity_pruning_path(x_t,y_t)
alphas=path.ccp_alphas
params={'ccp_alpha':alphas}
ndt=GridSearchCV(dt,param_grid=params,n_jobs=-1,scoring='accuracy',cv=10)  # Finding Best Alpha
ndt.fit(x_t,y_t)
print(ndt.best_params_)
print(ndt.best_score_)
best_alpha=ndt.best_params_['ccp_alpha']

dt=DecisionTreeClassifier(ccp_alpha=best_alpha)
params={'criterion':['gini','entropy'],'min_samples_split':list(np.arange(2,51)),'min_samples_leaf':list(np.arange(2,51)),
        'max_features':['sqrt','log2',None]}
ndt=RandomizedSearchCV(dt,param_distributions=params,scoring='accuracy',random_state=42,n_jobs=-1,cv=10)
ndt.fit(x_t,y_t)
print(ndt.best_params_)
print(ndt.best_score_)
ndt=ndt.best_estimator_

plt.figure(figsize=(20, 15))
plot_tree(ndt,filled=True,feature_names=x_t.columns, class_names=['Low', 'Medium','High'],rounded=True)
plt.show()

pred1_t=ndt.predict(x_t)
pred1=ndt.predict(x_te)  #Predicting on the testing set
print('Decision Tree Classifier Training Accuracy: ',accuracy_score(y_t,pred1_t))
print('Decision Tree Classifier Testing Accuracy: ',accuracy_score(y_te,pred1))
print('\nClassification Report for Decision Tree Classifier Classifier:\n\n ',classification_report(y_te,pred1))
cmdt_t=confusion_matrix(y_t,pred1_t)
cm_t.append(cmdt_t)
cmdt=confusion_matrix(y_te,pred1)
cm.append(cmdt)

"""# Random Forest"""

rf=RandomForestClassifier()
params={'criterion':['gini','entropy'],'min_samples_split':list(np.arange(2,41)),'min_samples_leaf':list(np.arange(2,41)),
        'max_features':['sqrt','log2',None],'n_estimators':[1000]}
nrf=RandomizedSearchCV(rf,param_distributions=params,scoring='accuracy',random_state=42,n_jobs=-1,cv=10)
nrf.fit(x_t,y_t)
print(nrf.best_params_)
print(nrf.best_score_)
nrf=nrf.best_estimator_

pred2_t=nrf.predict(x_t)
pred2=nrf.predict(x_te)  #Predicting on the testing set
print('Random Forest Classifier Training Accuracy: ',accuracy_score(y_t,pred2_t))
print('Random Forest Classifier Testing Accuracy: ',accuracy_score(y_te,pred2))
print('\nClassification Report for Random Forest Classifier Classifier:\n\n ',classification_report(y_te,pred2))
cmrf_t=confusion_matrix(y_t,pred2_t)
cm_t.append(cmrf_t)
cmrf=confusion_matrix(y_te,pred2)
cm.append(cmrf)

"""# AdaBoost"""

ada=AdaBoostClassifier(algorithm='SAMME')
params={
    'n_estimators': [300],
    'learning_rate': np.arange(0.01, 2.01, 0.01),}

nada=RandomizedSearchCV(ada,param_distributions=params,cv=10,n_jobs=-1,scoring='accuracy')
nada.fit(x_t ,y_t)
print(nada.best_params_)
print(nada.best_score_)
nada=nada.best_estimator_

pred3_t=nada.predict(x_t)
pred3=nada.predict(x_te)
print('AdaBoost Classifier Training Accuracy: ',accuracy_score(y_t,pred3_t))
print('Adaboost Classifier Testing Accuracy: ',accuracy_score(y_te,pred3))
print('\nClassification Report for Random Forest Classifier Classifier:\n\n ',classification_report(y_te,pred2))
cmada_t=confusion_matrix(y_t,pred3_t)
cm_t.append(cmada_t)
cmada=confusion_matrix(y_te,pred3)
cm.append(cmada)

"""# Training and Testing Scores for all the Models"""

tdf=pd.DataFrame({'Classification Algorithms':['KNN','Decision Tree Classifier','Random Forest Classifier','AdaBoost Classifier'],
                  'Training Accuracy':[accuracy_score(y_t,pred_t),accuracy_score(y_t,pred1_t),accuracy_score(y_t,pred2_t),accuracy_score(y_t,pred3_t)],
                  'Training Precision':[precision_score(y_t,pred_t,average='macro'),precision_score(y_t,pred1_t,average='macro'),precision_score(y_t,pred2_t,average='macro'),precision_score(y_t,pred3_t,average='macro')],
                  'Training Recall':[recall_score(y_t,pred_t,average='macro'),recall_score(y_t,pred1_t,average='macro'),recall_score(y_t,pred2_t,average='macro'),recall_score(y_t,pred3_t,average='macro')],
                  'Training F1 Score':[f1_score(y_t,pred_t,average='macro'),f1_score(y_t,pred1_t,average='macro'),f1_score(y_t,pred2_t,average='macro'),f1_score(y_t,pred3_t,average='macro')]})
tdf

tedf=pd.DataFrame({'Classification Algorithms':['KNN','Decision Tree Classifier','Random Forest Classifier','AdaBoost Classifier'],
                  'Testing Accuracy':[accuracy_score(y_te,pred),accuracy_score(y_te,pred1),accuracy_score(y_te,pred2),accuracy_score(y_te,pred3)],
                  'Testing Precision':[precision_score(y_te,pred,average='macro'),precision_score(y_te,pred1,average='macro'),precision_score(y_te,pred2,average='macro'),precision_score(y_te,pred3,average='macro')],
                  'Testing Recall':[recall_score(y_te,pred,average='macro'),recall_score(y_te,pred1,average='macro'),recall_score(y_te,pred2,average='macro'),recall_score(y_te,pred3,average='macro')],
                  'Testing F1 Score':[f1_score(y_te,pred,average='macro'),f1_score(y_te,pred1,average='macro'),f1_score(y_te,pred2,average='macro'),f1_score(y_te,pred3,average='macro')]})
tedf

mods=['KNN','Decision Tree Classifier','Random Forest Classifier','AdaBoost Classifier']
labels=['Low','Medium','High']
fig,ax=plt.subplots(2,2,figsize=(15,5))
r=0
c=0
for i,mod in enumerate(mods):
    sns.heatmap(cm_t[i],annot=True,fmt='d',ax=ax[r][c],xticklabels=labels,yticklabels=labels)
    ax[r][c].set_title(mod)
    c+=1
    if(c==2):
        r+=1
        c=0
plt.suptitle('Confusion Matrices for Models on Training Set')
plt.tight_layout()
plt.show()

mods=['KNN','Decision Tree Classifier','Random Forest Classifier','AdaBoost Classifier']
labels=['Low','Medium','High']
fig,ax=plt.subplots(2,2,figsize=(15,5))
r=0
c=0
for i,mod in enumerate(mods):
    sns.heatmap(cm[i],annot=True,fmt='d',ax=ax[r][c],xticklabels=labels,yticklabels=labels)
    ax[r][c].set_title(mod)
    c+=1
    if(c==2):
        r+=1
        c=0
plt.suptitle('Confusion Matrices for Models on Testing Set')
plt.tight_layout()
plt.show()

"""# Conclusion


*   Best Performers: Random Forest and AdaBoost classifiers achieved perfect scores in both training and testing, indicating excellent generalization
*   KNN Performance: KNN also performed well, with perfect testing metrics, but slightly lower training scores compared to the best models. It is still one of the best performers as it generalized the data extreamly well
*Decision Tree Limitations: The Decision Tree Classifier showed strong training performance but lower testing accuracy. We unfortunatly Overfit :(


"""